{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'C:\\\\\\\\Users\\\\\\\\Dror\\\\\\\\Downloads\\\\\\\\madelon\\\\\\\\madelon_train.data' does not exist: b'C:\\\\\\\\Users\\\\\\\\Dror\\\\\\\\Downloads\\\\\\\\madelon\\\\\\\\madelon_train.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f1cded6759c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0msetup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresultsFolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataFolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugmented_image_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder_mark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfolder_definition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"madelon_train.data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"madelon_train.labels\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mdata_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"madelon_valid.data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'C:\\\\\\\\Users\\\\\\\\Dror\\\\\\\\Downloads\\\\\\\\madelon\\\\\\\\madelon_train.data' does not exist: b'C:\\\\\\\\Users\\\\\\\\Dror\\\\\\\\Downloads\\\\\\\\madelon\\\\\\\\madelon_train.data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "def folder_definition(setup):\n",
    "\n",
    "    if (os.name == 'nt'):\n",
    "        if setup==1:\n",
    "            folder = r\"C:\\\\Users\\\\dl25365\\\\works\\\\madelon\\\\\"\n",
    "            folder = r\"i:\\\\works\\\\jupyternotebooks\\\\madelon\\\\\"\n",
    "        else:\n",
    "            folder = r\"C:\\\\Users\\\\Dror\\\\Downloads\\\\madelon\\\\\"\n",
    "        resultsFolder = folder + \"\\\\results1\\\\\"\n",
    "        dataFolder = folder + \"\\\\results\\\\\"\n",
    "        image_path = folder + '\\\\data\\\\' \n",
    "        folder_mark = '\\\\'\n",
    "        augmented_image_path = folder + '\\\\augmented\\\\'\n",
    "    else:\n",
    "        folder = r\"/Users/drorlederman/Documents/works/madelon/\"\n",
    "        resultsFolder = folder + \"/results1/\"\n",
    "        dataFolder = folder + \"/results/\"\n",
    "        image_path = folder + '/data/' \n",
    "        augmented_image_path = folder + '/augmented/' \n",
    "        folder_mark = '/'\n",
    "    return folder, resultsFolder, dataFolder, image_path, augmented_image_path, folder_mark\n",
    "\n",
    "setup = 0\n",
    "folder, resultsFolder, dataFolder, image_path, augmented_image_path, folder_mark = folder_definition(setup)\n",
    "data = pd.read_csv(folder+\"madelon_train.data\", header=None, delimiter=' ')\n",
    "labels = pd.read_csv(folder+\"madelon_train.labels\", header=None)  \n",
    "data_valid = pd.read_csv(folder+\"madelon_valid.data\", header=None, delimiter=' ')\n",
    "labels_valid = pd.read_csv(folder+\"madelon_valid.labels\", header=None)  \n",
    "#indices = np.load('indices.npy')\n",
    "indices = []\n",
    "data_merged = pd.concat([data, data_valid])\n",
    "labels_merged = pd.concat([labels, labels_valid])\n",
    "\n",
    "data = data_merged.copy().reset_index(drop=True)\n",
    "labels = labels_merged.copy().reset_index(drop=True)\n",
    "labels[labels == -1] = 0\n",
    "data = data.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "sys.path.insert(0, \"C:\\\\Users\\\\dl25365\\\\works\\\\fixedcouponpricer\\\\fixedcouponpricer\")\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 64) (1500, 64) (1500, 64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "weights = [1,2,3,4]\n",
    "no_of_samples = 10000\n",
    "#cols = list('ABCDEFGHJIKLMNOP')\n",
    "cols = [str(i) for i in range(0,64)]\n",
    "data_vect = pd.DataFrame(np.random.randn(no_of_samples, len(cols)), columns=cols)\n",
    "y = weights[0] * data_vect['0'] + weights[1] * data_vect['1'] + weights[2]*data_vect['2'] + weights[3] * data_vect['3'] \n",
    "labels = pd.DataFrame(columns={'dec'})\n",
    "labels['dec'] = (y > 0)\n",
    "labels[labels['dec']==True] = 1\n",
    "labels[labels['dec']==False] = 0\n",
    "x_train_vect, x_val_vect, x_test_vect, y_train, y_val, y_test = train_val_test_split(data_vect.to_numpy(), labels.to_numpy())\n",
    "print(x_train_vect.shape, x_val_vect.shape, x_test_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_classes = 2\n",
    "epochs = 50\n",
    "max_rows = 1000\n",
    "no_of_batches = 0\n",
    "no_of_scrambles = 0\n",
    "augment = False\n",
    "convert_rgb = True\n",
    "# input image dimensions\n",
    "fv = StructDataTransformer()\n",
    "fv.fit(data_vect.to_numpy(), labels.to_numpy())\n",
    "img_rows, img_cols = fv.image_dim, fv.image_dim\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = train_val_test_split(fv.imgs, labels.to_numpy())\n",
    "fv.augment(x_train, y_train, no_of_batches, no_of_scrambles)\n",
    "fv.combine_data(x_train, y_train)\n",
    "x_train = fv.combined_training_imgs.copy()\n",
    "y_train = fv.combined_training_labels.copy()\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "print(fv.combined_training_imgs.shape, fv.combined_training_labels.shape)\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, input_shape = pre_process(x_train, y_train, x_val, y_val, x_test, y_test, num_classes)\n",
    "#pre_process(x_train, y_train, x_val, y_val, x_test, y_test, num_classes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = apply_training(x_train, y_train, x_val, y_val, batch_size, epochs, input_shape, num_classes, augment)\n",
    "score = apply_testing(model, x_test, y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9393333333333334\n",
      "Test set accuracy: 0.9473333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "#print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "clf=RandomForestClassifier(n_estimators=1000)\n",
    "clf.fit(x_train_vect,y_train)\n",
    "y_pred_val=clf.predict(x_val_vect)\n",
    "y_pred_test=clf.predict(x_test_vect)\n",
    "print(\"Validation accuracy:\",metrics.accuracy_score(y_val, y_pred_val))\n",
    "print(\"Test set accuracy:\",metrics.accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_val = apply_testing(model, x_val, y_val)\n",
    "score_test = apply_testing(model, x_test, y_test)\n",
    "print('Validation loss: ', score_val[0], ' Test loss:', score_test[0])\n",
    "print('Validation accuracy: ', score_val[1], 'Test accuracy:', score_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from multiprocessing import Pool\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def train_val_test_split(data_vect, labels, train_size=0.7, val_size=0.15, test_size=0.15):\n",
    "    x_train_vect, x_test_vect, y_train, y_test = train_test_split(data_vect, labels, test_size=(val_size+test_size))\n",
    "    x_val_vect, x_test_vect, y_val, y_test = train_test_split(x_test_vect, y_test, test_size=test_size/(1-train_size))\n",
    "    return x_train_vect, x_val_vect, x_test_vect, y_train, y_val, y_test\n",
    "\n",
    "def scale(X, x_min, x_max, X_min=[], X_max=[]):\n",
    "    if (len(X_min) == 0):\n",
    "        X_min = X.min(axis=0)\n",
    "    if (len(X_max) == 0):\n",
    "        X_max = X.max(axis=0)\n",
    "    nom = (X-X_min)*(x_max-x_min)\n",
    "    denom = X_max - X_min\n",
    "    denom[denom==0] = 1\n",
    "    return (x_min + nom/denom), X_min, X_max\n",
    "\n",
    "def scramble_image(img):\n",
    "    no_rows, no_cols, no_of_channels = img.shape[0], img.shape[1], img.shape[2]\n",
    "    num_pix = no_rows * no_cols * no_of_channels\n",
    "    img_flat = img.reshape(num_pix, -1)\n",
    "    perm_vect = np.random.permutation(num_pix)\n",
    "    return img_flat[perm_vect].reshape(no_rows, no_cols, no_of_channels)\n",
    "\n",
    "def to_rgb(img):\n",
    "    return np.repeat(x[..., np.newaxis], 3, -1)    \n",
    "    \n",
    "class StructDataTransformer(TransformerMixin):\n",
    "    \"\"\"Transforms structured data\n",
    "    \"\"\"\n",
    "    def __init__(self, max_std=None, to_codes=None, ignore=None, randomize_features=False, indices=[], convert_rgb=False):\n",
    "        self.max_std = max_std\n",
    "        if not to_codes:\n",
    "            to_codes = []\n",
    "        if not ignore:\n",
    "            ignore = []\n",
    "        self.ignore_ = ignore\n",
    "        self.image_dim = 0 \n",
    "        self.padded_vect = []\n",
    "        self.img = []\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        self.augmented_imgs = []\n",
    "        self.augmented_labels = []\n",
    "        self.combined_training_imgs = []\n",
    "        self.combined_training_labels = []\n",
    "        self.means = []\n",
    "        self.stds = []\n",
    "        self.imgs_normalized = []\n",
    "        self.randomize_features = randomize_features\n",
    "        self.new_order = []\n",
    "        self.indices = indices\n",
    "        self.convert_rgb = convert_rgb\n",
    "        return\n",
    "    \n",
    "    def randomize_features(self, X):\n",
    "        # function accepts vector\n",
    "        if self.randomize_features and len(self.new_order) == 0:\n",
    "            self.new_order = np.random.permutation(X.shape[1])\n",
    "        return\n",
    "    \n",
    "    def permute(self, X, order):\n",
    "        X = X[:,order]\n",
    "        return X\n",
    "    \n",
    "    def combine_data(self, x_train, y_train):\n",
    "        if (len(self.augmented_imgs) > 0) & (len(self.augmented_labels) > 0):\n",
    "            self.combined_training_imgs = np.concatenate((x_train, self.augmented_imgs), axis=0)\n",
    "            self.combined_training_labels = np.concatenate((y_train, self.augmented_labels), axis=0)\n",
    "        else:\n",
    "            self.combined_training_imgs = x_train.copy()\n",
    "            self.combined_training_labels = y_train.copy()\n",
    "        return\n",
    "    \n",
    "    def augment(self, x_train, y_train, no_of_batches=20, no_of_scrambles=0):\n",
    "        if (no_of_batches > 0):\n",
    "            datagen = ImageDataGenerator(\n",
    "                featurewise_center=True,\n",
    "                featurewise_std_normalization=True,\n",
    "                width_shift_range=0.2,\n",
    "                height_shift_range=0.2,\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True)\n",
    "            datagen.fit(x_train)        \n",
    "            train_generator = datagen.flow(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                seed=42)\n",
    "            i = 0\n",
    "            self.augmented_imgs = []\n",
    "            self.augmented_labels = []\n",
    "            for batch in train_generator:\n",
    "                i += 1\n",
    "                x_batch, y_batch = train_generator.next()\n",
    "                x_batch = np.expand_dims(x_batch, axis=0)\n",
    "                y_batch = np.expand_dims(y_batch, axis=0)\n",
    "                if (len(self.augmented_imgs) == 0):\n",
    "                    for row_batch, row_label in zip(x_batch, y_batch):\n",
    "                        self.augmented_imgs = row_batch\n",
    "                        self.augmented_labels = row_label\n",
    "                else:\n",
    "                    for row_batch, row_label in zip(x_batch, y_batch):\n",
    "                        self.augmented_imgs = np.append(self.augmented_imgs, row_batch, axis=0) \n",
    "                        self.augmented_labels = np.append(self.augmented_labels, row_label, axis=0)\n",
    "                if i > no_of_batches:\n",
    "                    break\n",
    "        if (no_of_scrambles > 0):\n",
    "            for row, row_label in zip(x_train, y_train):\n",
    "                for i in range(0, no_of_scrambles):\n",
    "                    img = scramble_image(row)\n",
    "                    img = np.expand_dims(img, axis=0)\n",
    "                    img_label = np.expand_dims(row_label, axis=0)\n",
    "                    if (len(img_label.shape) == 1):\n",
    "                        img_label = np.expand_dims(img_label, axis=0)\n",
    "                    if (len(self.augmented_imgs) == 0):\n",
    "                        self.augmented_imgs = img\n",
    "                        self.augmented_labels = img_label\n",
    "                    else:\n",
    "                        self.augmented_imgs = np.append(self.augmented_imgs, img, axis=0) \n",
    "                        #print(self.augmented_labels.shape, img_label.shape)\n",
    "                        self.augmented_labels = np.append(self.augmented_labels, img_label, axis=0)\n",
    "        return self\n",
    "    \n",
    "    def calc_padding_order(self, X):\n",
    "        return (self.image_dim ** 2 - X.shape[1])\n",
    "    \n",
    "    def calc_stats(self):\n",
    "        self.means = self.imgs.mean(axis=(0), dtype='float64')\n",
    "        self.stds = self.imgs.std(axis=(0), dtype='float64')\n",
    "        return\n",
    "    \n",
    "    def pad(self, X):\n",
    "        self.image_dim = math.ceil(math.sqrt(X.shape[1]))\n",
    "        #self.image_dim = 28      # TBD\n",
    "        no_of_zeros = self.calc_padding_order(X)\n",
    "        X_padded = np.pad(X, ((0,0),(0, no_of_zeros)), 'edge') # 'constant', constant_values=(0))\n",
    "        return X_padded\n",
    "    \n",
    "    def fit_func(self, X, y):\n",
    "        img = X.reshape((self.image_dim, self.image_dim, 1), order='F')\n",
    "        if self.convert_rgb:\n",
    "            img = to_rgb(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        if (len(self.imgs) == 0):\n",
    "            self.imgs = img\n",
    "            self.labels = y\n",
    "        else:\n",
    "            self.imgs = np.append(self.imgs, img, axis=0)\n",
    "            self.labels = np.append(self.labels, y, axis=0)\n",
    "        return self\n",
    "\n",
    "    def to_image(self, X, y):\n",
    "        if self.randomize_features:\n",
    "            X = self.permute(X, self.new_order)\n",
    "        if len(self.indices) > 0:\n",
    "            #print(X[:10,:10])\n",
    "            X = self.permute(X, self.indices)\n",
    "            #print(X[:10,:10])\n",
    "        \n",
    "        X_padded = self.pad(X)\n",
    "        for row, row_label in zip(X_padded, y):\n",
    "            self.fit_func(row, row_label)\n",
    "        return \n",
    "    \n",
    "    def normalize(self, x_train):\n",
    "        #self.imgs = (self.imgs - self.means) / (self.stds + 10 ** (-7))\n",
    "        #self.imgs = (self.imgs - self.means) / (self.stds + 10 ** (-7))\n",
    "        #X2.mean()\n",
    "\n",
    "        return scale(x_train, 0, 255)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # pad vector with zeros to get the appropriate length\n",
    "        #X = scale(X, 0, 255)\n",
    "        #X /= 255\n",
    "        self.to_image(X, y)\n",
    "        \n",
    "        # calculate statistics\n",
    "        self.calc_stats()\n",
    "        \n",
    "        # normalize\n",
    "        #self.normalize()\n",
    "        return\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        self.to_image(X)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "def pre_process(x_train, y_train, x_val, y_val, x_test, y_test, num_classes):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "        x_val = x_val.reshape(x_val.shape[0], 1, img_rows, img_cols)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "        x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n",
    "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_val = x_val.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train, X_min, X_max = scale(x_train, 0, 255)\n",
    "    x_val, _, _ = scale(x_val, 0, 255, X_min=X_min, X_max=X_max)\n",
    "    x_test, _, _ = scale(x_test, 0, 255, X_min=X_min, X_max=X_max)\n",
    "    x_train /= 255\n",
    "    x_val /= 255\n",
    "    x_test /= 255\n",
    "    # convert class vectors to binary class matrices\n",
    "    f = False\n",
    "    if f:\n",
    "        i = 0\n",
    "        for row in x_train:\n",
    "            x_train[i,:] = preprocess_input(row)\n",
    "            i = i + 1\n",
    "        i = 0\n",
    "        for row in x_val:\n",
    "            x_val[i,:] = preprocess_input(row)\n",
    "            i = i + 1\n",
    "        for row in x_test:\n",
    "            x_test[i,:] = preprocess_input(row)\n",
    "            i = i + 1\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test, input_shape\n",
    "\n",
    "def apply_training(x_train, y_train, x_val, y_val, batch_size, epochs, input_shape, num_classes, augment=False):\n",
    "    l2_reg = 0.0001\n",
    "    l1_reg = 0.0001\n",
    "    sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    adam = optimizers.Adam(lr=1e-3, epsilon = 1e-8, beta_1 = .9, beta_2 = .999)\n",
    "    # augmentation\n",
    "    #datagen = augmentation(x_train, y_train)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization()) \n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu',\n",
    "             kernel_regularizer=regularizers.l2(l2_reg),\n",
    "             activity_regularizer=regularizers.l1(l1_reg)))\n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    #print(x_train.shape, y_train.shape, x_test.shape, y_test.shape, augment)\n",
    "    #print(input_shape, x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "    #if augment:\n",
    "    #    history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "    #              verbose=1, \n",
    "    #              steps_per_epoch=math.ceil(x_train.shape[0]/ batch_size),\n",
    "    #              validation_data=(x_test, y_test))\n",
    "    #else:\n",
    "    history = model.fit(x_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  verbose=1,\n",
    "                  callbacks=[es],\n",
    "                  validation_data=(x_val, y_val))\n",
    "    return model, history\n",
    "\n",
    "def apply_training_resnet(x_train, y_train, x_test, y_test, batch_size, epochs, input_shape, num_classes, augment=False):\n",
    "    l2_reg = 0.0001\n",
    "    l1_reg = 0.0001\n",
    "    sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    adam = optimizers.Adam(lr=1e-4, epsilon = 1e-8, beta_1 = .9, beta_2 = .999)\n",
    "    # augmentation\n",
    "    #datagen = augmentation(x_train, y_train)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "    \n",
    "    model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    # add new classifier layers\n",
    "    flat1 = Flatten()(model.outputs)\n",
    "    class1 = Dense(1024, activation='relu')(flat1)\n",
    "    output = Dense(10, activation='softmax')(class1)\n",
    "    # define new model\n",
    "    model = Model(inputs=model.inputs, outputs=output)\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  verbose=1,\n",
    "                  callbacks=[es],\n",
    "                  validation_data=(x_test, y_test))\n",
    "    return model, history\n",
    "\n",
    "def apply_testing(model, x_test, y_test):\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 100\n",
    "max_rows = 1000\n",
    "no_of_batches = 0\n",
    "no_of_scrambles = 0\n",
    "augment = False\n",
    "convert_rgb = True\n",
    "# input image dimensions\n",
    "fv = StructDataTransformer(indices=indices, convert_rgb=convert_rgb)\n",
    "fv.fit(data.to_numpy(), labels.to_numpy())\n",
    "img_rows, img_cols = fv.image_dim, fv.image_dim\n",
    "x_train, x_test, y_train, y_test = train_test_split(fv.imgs, labels, test_size=0.33)\n",
    "fv.augment(x_train, y_train, no_of_batches, no_of_scrambles)\n",
    "fv.combine_data(x_train, y_train)\n",
    "print(fv.combined_training_imgs.shape, fv.combined_training_labels.shape)\n",
    "x_train = fv.combined_training_imgs.copy()\n",
    "y_train = fv.combined_training_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_rows, img_cols, 1)\n",
    "#x_train, y_train, x_test, y_test, input_shape = pre_process(x_train, y_train, x_test, y_test, num_classes)\n",
    "score = apply_testing(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apply_training(x_train, y_train, x_test, y_test, batch_size, epochs, input_shape, num_classes, augment)\n",
    "#model = apply_training_resnet(x_train, y_train, x_test, y_test, batch_size, epochs, input_shape, num_classes, augment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "augment = False\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "x_train, y_train, x_test, y_test, input_shape = pre_process(x_train, y_train, x_test, y_test)\n",
    "model, history = apply_training(x_train, y_train, x_test, y_test, batch_size, epochs, input_shape, num_classes, augment)\n",
    "score = apply_testing(model)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#cv2.imwrite(\"filename.png\", np.zeros((10,10)))\n",
    "ind = 0\n",
    "for row in fv.imgs:\n",
    "#for i in range(fv.imgs.shape[0]):\n",
    "    if labels.iloc[ind,0] == 0:\n",
    "        class_folder = folder_mark + '0' + folder_mark\n",
    "    else:\n",
    "        class_folder = folder_mark + '1' + folder_mark\n",
    "    file_name = image_path + class_folder + str(ind) + '.jpg'\n",
    "    print(file_name)\n",
    "    cv2.imwrite(file_name, row)\n",
    "    ind += 1\n",
    "    \n",
    "#https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "\n",
    "img = load_img('bird.jpg')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load an color image in grayscale\n",
    "img = cv2.imread('bird.jpg',0)\n",
    "no_rows = img.shape[0]\n",
    "no_cols = img.shape[1]\n",
    "num_pix = no_rows * no_cols\n",
    "img_flat = img.reshape(num_pix, -1)\n",
    "perm_vect = np.random.permutation(num_pix)\n",
    "img_ar = img_flat[perm_vect]\n",
    "img_ar2 = img_ar.reshape(no_rows, no_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array( [[ 1, 2, 3],\n",
    "                 [ 4, 2, 5]] )\n",
    "no_rows = img.shape[0]\n",
    "no_cols = img.shape[1]\n",
    "num_pix = no_rows * no_cols\n",
    "img_flat = img.reshape(num_pix, -1)\n",
    "perm_vect = np.random.permutation(num_pix)\n",
    "img_ar = img_flat[perm_vect]\n",
    "img_ar2 = img_ar.reshape(no_rows, no_cols)\n",
    "print(img)\n",
    "print(img_ar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#x_train /= 255\n",
    "#x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred=clf.predict(x_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
